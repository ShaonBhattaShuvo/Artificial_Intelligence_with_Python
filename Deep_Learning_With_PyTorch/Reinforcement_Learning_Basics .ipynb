{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88d540a",
   "metadata": {},
   "source": [
    "# Reinforecement Learning (RL)\n",
    "- RL deals with creating artificial agents that make decisions to solve complex tasks. \n",
    "- As a branch of Artificial Intelligence (AI), RL specializes in addressing control tasks, which are omnipresent in various domains. These tasks range from cognitive challenges like playing chess, where each move represents a decision toward victory, to dynamic activities such as playing video games or driving. The latter exemplifies how RL can be instrumental in developing autonomous systems, such as, autonomous robots for better productivity in industries, self-driving cars, data driven decision support system.\n",
    "- At the heart of RL is the principle that agents learn solely from experience. This learning process is driven by the interactions of the agent with its environment, where it continuously improves its performance based on the consequences of its actions. The goal of RL is to develop a strategy, or policy, that maximizes the cumulative reward for the agent, enabling it to make optimal decisions in complex environments. \n",
    "\n",
    "\n",
    "The idea behind Reinforcement Learning is that an agent (an AI) will learn from the environment by interacting with it (through trial and error) and receiving rewards (negative or positive) as feedback for performing actions. Learning from interactions with the environment comes from our natural experiences. \n",
    "\n",
    "Ultimately, the goal of RL is for the agent to learn how to navigate the environment to maximize a cumulative reward metric. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f7379",
   "metadata": {},
   "source": [
    "# Key Concepts of Reinforcement Learning\n",
    "\n",
    "### Agent\n",
    "The agent is the decision-maker in the RL framework. It learns to make decisions through interactions with the environment to achieve specific goals. \n",
    "\n",
    "Lets consider the context of a self-driving car, agent is the car's autonomous driving system. It learns to make decisions to navigate through traffic, follow road rules, and reach destinations safely and efficiently.\n",
    "\n",
    "### Environment: \n",
    "The environment represents the world in which the agent operates. It is the external system the agent interacts with, which responds to the agent's actions and presents new situations or states.\n",
    "\n",
    "For a self-driving car, the environment includes the roads, traffic signals, other vehicles, pedestrians, and weather conditions. The car's sensors and cameras continuously capture this information, allowing the car to understand and interact with its surroundings.\n",
    "\n",
    "### Action\n",
    "Actions are the set of decisions or moves the agent can make in a given state. The agent's choice of action influences its future state and the rewards it receives.\n",
    "\n",
    "Actions for a self-driving car might include accelerating, braking, steering, changing lanes, or adjusting the speed based on traffic conditions. Each action the car takes affects its immediate future, such as moving closer to its destination or avoiding potential hazards.\n",
    "\n",
    "- #### Action Space\n",
    "The Action space is the set of all possible actions in an environment. The actions can come from a discrete or continuous space:\n",
    "\n",
    "    - Discrete space: the number of possible actions is finite. For example, in chess, the basic actions can be summarized as moving a piece to a different square or capturing an opponent's piece on the board. \n",
    "    \n",
    "    - Continues space: the number of possible actions is infinite. For example, a Self Driving Car agent has an infinite number of possible actions since it can turn left 20°, 21,1°, 21,2°, honk, turn right 20°…\n",
    "\n",
    "### State \n",
    "The state represents the current situation or context in which the agent exists. It's a snapshot of the environment at a particular time point, providing the information the agent needs to decide its next action.\n",
    "\n",
    "The state for a self-driving car includes its current speed, GPS location, the condition of nearby vehicles (like their speed and proximity), traffic light status, and road conditions. This comprehensive set of data forms the basis on which the car's driving system makes decisions.\n",
    "\n",
    "- #### State vs Observation:\n",
    "\n",
    "    - State (s): is a complete description of the state of the world (there is no hidden information). In a fully observed environment.\n",
    "    In a chess game, we have access to the whole board information, so we receive a state from the environment. In other words, the environment is fully observed.\n",
    "    \n",
    "    - Observation(o): is a partial description of the state. In a partially observed environment.\n",
    "    In Autonomous Car, we are in a partially observed environment. We receive an observation since we only see a part of the whole enviroment.\n",
    "    \n",
    "\n",
    "### Reward \n",
    "Rewards are immediate feedback from the environment based on the actions taken by the agent. The goal of the agent is to maximize the cumulative reward over time, which guides its learning process. \n",
    "\n",
    "The reward for a self-driving car could be based on reaching the destination in the shortest time while maximizing passenger safety and comfort. This can include minimizing abrupt stops, avoiding accidents, obeying traffic laws, and ensuring a smooth ride. The car’s system learns to optimize these parameters to improve driving performance and safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6173e7",
   "metadata": {},
   "source": [
    "# The RL Problem Statement\n",
    "The problem statement in Reinforcement Learning revolves around finding a strategy, or policy, that maximizes the cumulative reward for the agent over time. The challenge lies in balancing immediate rewards with long-term gains, often requiring the agent to explore the environment to discover the most rewarding actions. The agent must learn from its interactions, continually improving its policy based on the outcomes of its actions to achieve optimal performance in the given environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aade33",
   "metadata": {},
   "source": [
    "# Fundamentals of RL\n",
    "\n",
    "### Exploration vs. Exploitation\n",
    "- Exploration is about discovering new knowledge by trying different actions to see their outcomes.\n",
    "- Exploitation uses existing knowledge to make decisions that seem to offer the best reward.\n",
    "\n",
    "Balancing these two is crucial for effective learning in RL. Too much exploration can lead to inefficiency, while too much exploitation can prevent finding more optimal strategies. Therefore, we must define a rule that helps to handle this trade-off. \n",
    "\n",
    "- Real life example:\n",
    "    - Exploitation: Ordering the same dish each time you visit a particular restaurant because you know you like it, risking missing out on other delicious menu items.\n",
    "\n",
    "    - Exploration: Trying different dishes each time you visit the restaurant, risking that you might not enjoy the new dish as much but with the chance of finding another favorite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5ec56",
   "metadata": {},
   "source": [
    "\n",
    "### Markov property\n",
    "Reinforcement Learning problem is in essenceMarkov Property implies that our agent needs only the current state to decide what action to take and not the history of all the states and actions they took before. \n",
    "\n",
    "Reinforcement Learning problem is in essence, a Markov Process (or Markov Chain), meaning the transition between states only depends on the immediate previous state — assuming each state encapsulates temporal information of all previous states — and the transition is typically stochastic.\n",
    "### The Markov Decision Process (MDP)\n",
    "A mathematical framework used to describe an environment in Reinforcement Learning (RL). \n",
    "An MDP is defined by four main components:  a set of states (S), actions (A), transition probabilities (P), and rewards (R). It provides a framework for modeling decision-making situations.\n",
    "\n",
    "- States (S) represent the configurations of the agent and its environment.\n",
    "- Actions (A) are the choices available to the agent.\n",
    "- Transition probabilities (P), $P(s_{t+1}∣s_t ,a_t)$, define the likelihood of moving from one state to another after an action.\n",
    "- Rewards (R), $R(s_t,a_t)$, are the feedback the agent receives after taking an action.\n",
    "\n",
    "We can perfectly describe a control task if we know these four elements (S, A, P, R). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19a492",
   "metadata": {},
   "source": [
    "### Policy and Value Functions\n",
    "- A policy \\($π$\\) is a strategy that maps states to actions, $π(a∣s)$, indicating the probability of taking action \\( a \\) in states \\( s \\). [In case of deterministic, $π(s)$ directly maps a specific action (a) , for that state (s)] \n",
    "\n",
    "- A value function estimates how good it is to be in a state or to take an action in that state.\n",
    "\n",
    " - State-value function \\( V(s) \\): Expected return starting from state \\( s \\).\n",
    "  $$ V_π(s) = \\mathbb{E}[G_t | S_t = s] $$\n",
    " - Action-value function \\( Q(s, a) \\): Expected return starting from state \\( s \\) and taking action \\( a \\).\n",
    "  $$ Q_π(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a] $$\n",
    "  \n",
    "##### Key Differences\n",
    "- Direct vs. Indirect Learning: Policy-based methods learn the policy directly, while in Value-based methods, the input is also the current state of the environment. However, rather than directly outputting an action, these methods estimate the value of each state (or state-action pair), which indirectly determines the best action.\n",
    "- Nature of Output: The immediate output of policy-based methods is the action itself (or a distribution over actions), while for value-based methods, it’s a value representing the expected return, which then guides action selection.\n",
    "- Convergence and Stability: Policy-based methods can handle continuous action spaces and can converge to a local optimum more smoothly. Value-based methods, especially with function approximators like neural networks, can be prone to overestimation and instability but are typically more straightforward to implement and understand.\n",
    "\n",
    "#### Reward, Return, and Discounting : \n",
    "Reward $(R_t)$ is the immediate result that our action produces.\n",
    "\n",
    "The return (G) is the total accumulated reward the agent receives over time, often discounted to value immediate rewards more than distant ones. \n",
    "- Return \\( $G_t$ \\): Total discounted reward from time \\( t \\)\n",
    "  $$ G_t  = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3}+... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$\n",
    "  where \\( $\\gamma$ \\) is the discount factor. This factor ranges between 0 and 1 (most of the time between 0.95 and 0.99). Where a value closer to 0 places more emphasis on immediate rewards, while a value closer to 1 gives more importance to future rewards.\n",
    "  \n",
    " Let's say an agent receives rewards of 5, 10, and 15 at time steps \\( t+1 \\), \\( t+2 \\), and \\( t+3 \\) respectively, with a discount factor \\( $\\gamma$ \\) of 0.9. The discounted return \\( $G_t$ \\) at time \\( t \\) would be calculated as follows:\n",
    "\n",
    "- Immediate reward at \\( t+1 \\): 5\n",
    "- Discounted reward at \\( t+2 \\): \\( 10 \\times 0.9 = 9 \\)\n",
    "- Discounted reward at \\( t+3 \\): \\( 15 \\times 0.9^2 = 12.15 \\)\n",
    "\n",
    "So, the total return \\( $G_t$ \\) would be: \\( $G_t$ = 5 + 9 + 12.15 = 26.15 \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab6f12",
   "metadata": {},
   "source": [
    "### The Bellman Equations\n",
    "- RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).\n",
    "\n",
    "The Bellman equations provide a recursive relationship for value functions in MDPs. They break down the value of the current state into the immediate reward plus the value of the next state, discounted by the discount factor. This relationship is fundamental in computing the optimal policy and value functions.\n",
    "\n",
    "- ###### For state-value function: \n",
    "This equation states that the value of a state is the expected reward from the next action plus the discounted value of the next state.\n",
    "  $$ V(s) = \\mathbb{E}[R_{t+1} + \\gamma V(S_{t+1}) | S_t = s] $$\n",
    "  How its derived? \n",
    "  Using State-value function we know, \n",
    "  $$ V(s) = \\mathbb{E}[G_t | S_t = s] $$\n",
    "  $$ V(s) = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3}+... | S_t = s] $$\n",
    "  $$ V(s) = \\mathbb{E}[R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3}+...) | S_t = s] $$\n",
    "  $$ V(s) = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s]$$\n",
    "  $$ V(s) = \\mathbb{E}[R_{t+1} + \\gamma V(s_{t+1}) | S_t = s]$$\n",
    "      \n",
    "- ###### For action-value function: \n",
    "This expresses that the value of taking an action in a state is the expected immediate reward plus the discounted value of the subsequent state, assuming the best future action is chosen.\n",
    "  $$ Q(s, a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') | S_t = s, A_t = a] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe166c",
   "metadata": {},
   "source": [
    "In summary, these fundamental concepts of RL are interconnected and essential for developing an efficient learning process. The agent learns to navigate its environment (using MDP), balance risk and reward (exploration vs. exploitation), evaluate its decisions (policy and value functions), and plan for the future (reward, return, and discounting) through iterative processes governed by mathematical principles like the Bellman equations. \n",
    "\n",
    "In the context of a self-driving car, these concepts come together as follows: \n",
    "- The car (agent) explores various routes (actions) and learns from each journey (experience).\n",
    "- Its environment includes the road conditions, traffic, and weather.\n",
    "- The car’s decisions (policy) are based on maximizing safety and efficiency (rewards), considering immediate conditions (state-value) and future predictions (action-value).\n",
    "- The Bellman equations help in updating the car’s strategy to improve decision-making over time, ensuring it learns to navigate optimally in its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea2963",
   "metadata": {},
   "source": [
    "# Types of Task in MDP\n",
    "\n",
    "### Episodic task\n",
    "Episodic tasks have a clear end state (a terminal state). The agent experiences the environment in segments called episodes. An episode creates a list of States, Actions, Rewards, and new States. An example of an episodic task is a chess game where each game is an episode, and the terminal state is reached when one player wins or the game ends in a draw.\n",
    "\n",
    "### Continuing Tasks\n",
    "Continuing tasks go on without a defined endpoint or terminatal state. The agent keeps interacting with the environment indefinitely. For instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd797ed7",
   "metadata": {},
   "source": [
    "# Model-Based vs Model-Free Approches\n",
    "\n",
    "### Model-Based Approaches  \n",
    "Model-Based RL involves the agent having or learning a model of the environment, which it uses to simulate outcomes and plan actions. This model predicts the next state and reward for each action taken in a given state. We can think of Model-Based RL like using a map and planning a journey before start driving. Here, the \"map\" is the model of the environment. \n",
    "   - What it does: In Model-Based RL, the learning agent utilizes a model that was previously learned to accomplish a task. The agent uses a model (like a map or guide) of how things work in the environment. This model helps the agent predict what will happen next after taking certain actions, similar to how we might use a map to foresee the route and obstacles on a road trip. \n",
    "\n",
    "   - Why it's useful: It's efficient because the agent can think ahead and make decisions without having to actually try everything in real life. Just as planning your route on a map can save you time and fuel, the agent can save effort and reduce mistakes by using its model to plan.\n",
    "    \n",
    "Example: In a self-driving car, using Model-Based RL would be like using the car's GPS and sensors to simulate and decide the best path to avoid traffic jams and reach the destination quickly.\n",
    "\n",
    "#### Characteristics:\n",
    "\n",
    "- Planning: The agent can plan by considering future states and rewards without directly interacting with the environment.\n",
    "- Sample Efficiency: Typically requires fewer interactions with the environment because the agent can learn from simulated experiences.\n",
    "\n",
    "It's about thinking ahead and being prepared.\n",
    "\n",
    "### Model -Free Approches\n",
    "Model-Free Reinforcement Learning (RL) involves learning from direct interaction with the environment, focusing on improving the policy or value function without explicitly predicting future states or rewards. Think of Model-Free RL as exploring a new city without a map, learning your way around through experience.\n",
    "\n",
    "   - What it does: In Model-Free RL, the agent learns directly from the interaction with the environment without forming an explicit model of the environment's dynamics. It doesn’t have a map or model to predict what will happen next. Instead, it learns from the outcome of its actions, similar to how we might learn to navigate a new city by walking around and observing landmarks.\n",
    "   - Why it's useful: It's straightforward because the agent directly learns from real experiences, which is powerful in situations where it's hard to predict everything in advance or when no accurate map is available.\n",
    "\n",
    "Example: For a Chess Game AI, Model-Free RL would involve learning how to play the game better by repeatedly playing it, understanding the rules, and adapting strategies based on wins or losses, without simulating the game's outcomes in advance (which is not possible as we do not not what opponent player would do after each move).\n",
    "\n",
    "#### Characteristics:\n",
    "\n",
    "- Direct Learning: The agent learns the value of actions and states through trial and error, directly from real experiences.\n",
    "- No Planning: The agent doesn't plan future actions based on a model but instead learns a policy or value function over time.\n",
    "\n",
    "It's about learning from doing.\n",
    "\n",
    "\n",
    "## Knowledge Accumulation (Model-Based vs Model-Free): \n",
    "While both approaches aim to maximize the cumulative reward, they differ in how they accumulate knowledge. Model-Free methods improve their policies or value functions based on direct experience, whereas Model-Based methods refine their model of the environment and use it for planning.\n",
    "- Model-Based RL is favored in scenarios where constructing a model of the environment is feasible and can significantly reduce the sample complexity, like in controlled environments or where simulations can effectively predict real-world outcomes.\n",
    "    - Project Example (Model-Based): Google used a Model-Based approach to reduce energy consumption in its data centers. By modeling the cooling systems and data center dynamics, they were able to optimize energy use efficiently.\n",
    "\n",
    "\n",
    "- Model-Free RL dominates applications where it is difficult to model the environment accurately or where the environment is highly complex and dynamic, like in many games or real-time decision-making systems.\n",
    "  - Project Example (Model-Free): AlphaGo by DeepMind used a combination of Model-Free RL (with deep neural networks) and tree search methods to master the game of Go. It's primarily known for its Model-Free component where deep learning models were trained to evaluate Go positions and moves.\n",
    "  \n",
    "  \n",
    "# Hybrid Approch\n",
    "There are hybrid approaches that combine elements of both Model-Free and Model-Based methods. For example, an agent might use a learned model of the environment to simulate future states and rewards (Model-Based) and then apply RL within those simulated environments to make decisions (Model-Free).\n",
    "Such hybrid methods aim to leverage the predictive power of Model-Based approaches for planning and the robust, flexible learning mechanisms of Model-Free approaches like DQN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049d885",
   "metadata": {},
   "source": [
    "# RL Characteristics (Summary)\n",
    "- Trial-and-Error Learning: RL involves learning optimal behaviors through trial and error, where the agent experiments with actions to discover their effects on the environment.\n",
    "\n",
    "- Delayed Rewards: Unlike supervised learning, where immediate feedback is provided, RL often deals with scenarios where rewards are delayed, making it necessary to link actions to outcomes over time.\n",
    "\n",
    "- Sequential Decision Making: In RL, decisions are not isolated; each decision impacts future states and decisions, requiring the agent to consider long-term consequences.\n",
    "\n",
    "- Agent-Environment Interaction: The core of RL is the interaction between the agent and its environment. The agent takes actions based on the state of the environment, and these actions change the state, influencing future decisions.\n",
    "- Finding Optimal Policy to Solve RL Problems: \n",
    "    - By training the policy directly: policy-based methods.\n",
    "        - Policy-based Learning: RL is focused on finding a policy — a mapping from states to actions — that maximizes the cumulative reward for the agent.\n",
    "\n",
    "    - By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\n",
    "        - Value Estimation: An essential part of RL is estimating the value of different states and actions, helping the agent predict the expected future rewards and make informed decisions.\n",
    "\n",
    "- Balance between Exploration and Exploitation: RL requires a balance between exploring new actions to find more rewarding options and exploiting known actions that already yield rewards.\n",
    "\n",
    "- Model-based and Model-free Approaches: RL can be both model-based, where the agent has a model of the environment to simulate outcomes, and model-free, where the agent learns directly from interactions without a model of the environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87cdf1f",
   "metadata": {},
   "source": [
    "# Trajectory vs. Episode\n",
    "\n",
    "- Trajectory\n",
    "    - Elements that are generated when agent moves from one state to another. The specific sequence of movements and decisions the agent makes within that episode. \n",
    "\n",
    "- Episode \n",
    "    - Simply a trajectory that start at the intial state and end at terminal state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e04bb",
   "metadata": {},
   "source": [
    "# Finding a Solution in an MDP\n",
    "Finding a solution in a Markov Decision Process (MDP) refers to determining an optimal policy that maximizes the expected return (or minimizes cost) for an agent interacting with an environment over time. Here’s what this involves:\n",
    "\n",
    "## Defining a Solution in an MDP\n",
    "   1. Optimal Policy (π): \n",
    "       - A policy is a rule or strategy that the agent follows to decide which action to take in each state.\n",
    "       - An optimal policy $π^*$ maximizes the cumulative reward (or minimizes the cumulative cost) the agent receives over time.\n",
    "       - It dictates the best action to take in every state, considering both immediate and future rewards.\n",
    "\n",
    "\n",
    "   2. Value Functions:\n",
    "        - The solution also involves computing the value functions that estimate the expected return from each state or state-action pair under the optimal policy.\n",
    "           - State-Value Function $V^*(s)$: Gives the expected return when starting in state (s) and following the optimal policy thereafter.\n",
    "           - Action-Value Function $Q^∗(s,a)$: Provides the expected return from taking action (a) in state (s) and then following the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1bd19",
   "metadata": {},
   "source": [
    "## How to find a solution\n",
    "To solve Markov Decision Processes (MDPs), several methods can be employed depending on whether the environment model is known or unknown, and whether the state and action spaces are discrete or continuous. Here are the primary methods used to solve MDPs:\n",
    "### Dynamic Programming (DP)\n",
    "Dynamic Programming is used when the MDP's model (states, actions, transitions, and rewards) is fully known. DP methods require a complete and accurate model of the environment.\n",
    "\n",
    "- Value Iteration: Iteratively updates the value of each state until the values converge to the optimal values. The policy is then derived from these values.\n",
    "- Policy Iteration: Involves two steps: policy evaluation, where the value of the current policy is calculated, and policy improvement, where the policy is updated based on the evaluated values.\n",
    "\n",
    "### Monte Carlo Methods\n",
    "Monte Carlo methods are used when the model is unknown and are based on learning from episodes. These methods do not require knowledge of transition probabilities and rewards in advance.\n",
    "\n",
    "- Process: Monte Carlo methods estimate the value function based on the average returns from multiple episodes. Policies are improved by exploring different actions and observing the outcomes.\n",
    "\n",
    "### Temporal Difference (TD) Learning\n",
    "TD Learning is a combination of Monte Carlo and Dynamic Programming features. It can learn directly from raw experience without a model and update estimates based in part on other learned estimates.\n",
    "\n",
    "- Q-Learning: A model-free method that learns the value of the best action to take in a given state (action-value function) without requiring a model of the environment.\n",
    "- SARSA (State-Action-Reward-State-Action): Another model-free method that updates the action-value function based on the action taken and the reward received, following the current policy.\n",
    "\n",
    "### Model-Based RL\n",
    "When partial knowledge of the model is available, or it can be learned, Model-Based RL methods can be used. These methods involve learning a model of the environment and using it to make decisions.\n",
    "\n",
    "- Simulating Outcomes: The agent uses the learned model to simulate the outcomes of actions and decide the best course of action based on those simulations.\n",
    "\n",
    "### Approximate Solution Methods\n",
    "In large-scale MDPs, where exact methods like DP, Monte Carlo, or TD Learning are computationally infeasible, approximate solution methods are used.\n",
    "\n",
    "- Function Approximation: Techniques like neural networks, decision trees, or linear functions are used to approximate the value functions or policy functions.\n",
    "- Deep Reinforcement Learning: Combines deep neural networks with RL principles to handle high-dimensional state or action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345159f3",
   "metadata": {},
   "source": [
    "# On-Policy Methods Vs Off-Policy Methods\n",
    "\n",
    "### On-Policy Methods \n",
    "Learn the value of the policy that is currently being used to make decisions. This means the policy used to generate the behavior (action choices) is the same policy that is being evaluated and improved.\n",
    "\n",
    "##### Characteristics:\n",
    "\n",
    "- The learning agent evaluates and improves the policy that it uses to make decisions.\n",
    "- The agent learns from the actions it takes based on its current strategy.\n",
    "- Example: SARSA (State-Action-Reward-State-Action) is an on-policy method. It updates the action-value function based on the action taken by the current policy. If the policy decides to explore and take a less optimal action, SARSA will include the results of that action in its updates.\n",
    "\n",
    "### Off-Policy Methods\n",
    "Learn the value of a potentially different policy from the one used to generate the behavior. This allows the agent to learn about the optimal policy while following an exploratory or different policy.\n",
    "\n",
    "#### Characteristics:\n",
    "\n",
    "- The learning agent evaluates and improves a policy different from the one it uses to make decisions.\n",
    "The agent can learn from actions that are outside its current strategy, allowing it to explore a wider range of possibilities.\n",
    "- Example: Q-learning is an off-policy method. It updates the action-value function using the maximum reward that can be obtained from the next state, regardless of the action taken by the policy in use. This means Q-learning can learn the optimal policy even if it's exploring or following a suboptimal policy.\n",
    "\n",
    "###### Key Differences\n",
    "- Exploration: On-policy methods integrate the exploration into the policy being learned, meaning that the policy must be exploratory to learn well. Off-policy methods separate the exploration process from the policy being learned, allowing the policy to be greedy (optimal) while the behavior can still be exploratory.\n",
    "- Data Efficiency: Off-policy methods can be more data-efficient as they can learn from historical data collected from different policies, not just the current one.\n",
    "- Complexity and Risk: Off-policy methods can be more complex and might have higher variance in their updates, but they have the advantage of being able to learn from a diverse range of experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3d287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
