{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8651955e",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "PyTorch is an open-source machine learning library used for applications such as computer vision and natural language processing. It is known for its flexibility, ease of use, and as a powerful tool for research. Developed by Facebook's AI Research lab, PyTorch provides two high-level features: Tensor computation (like NumPy) with strong GPU acceleration and deep neural networks built on a tape-based autograd system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025938b",
   "metadata": {},
   "source": [
    "## Tensors: Definition, Creation, and Operations\n",
    "A tensor is a multi-dimensional array, which is a fundamental data structure in PyTorch. Tensors are similar to NumPy’s ndarrays, but they can also be used on a GPU to accelerate computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d6148f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2,  1, 10]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[0.0598, 0.5531, 0.7379],\n",
       "         [0.2318, 0.8570, 0.3808],\n",
       "         [0.7311, 0.3703, 0.0036]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with default floating-point data type:\n",
    "x = torch.tensor([2, 1, 10])\n",
    "\n",
    "# Create a 2x3 tensor filled with zeros:\n",
    "y = torch.zeros(2, 3)\n",
    "\n",
    "# Create a tensor with random numbers:\n",
    "z = torch.rand(3, 3)\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed44e52",
   "metadata": {},
   "source": [
    "### torch.rand(): \n",
    "Generates random numbers from a uniform distribution over the interval [0, 1). This means that each number within this range is equally likely to be generated. The numbers are spread evenly over the specified range, and we can expect the mean of the generated values to be close to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268efced",
   "metadata": {},
   "source": [
    "### torch.randn(): \n",
    "Generates random numbers from a standard normal (Gaussian) distribution with mean 0 and variance 1. This distribution is also known as the bell curve due to its shape. In a normal distribution, values are more likely to be closer to the mean (0 in this case) and less likely to be found at the extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2408aacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform random tensor: tensor([[0.6898, 0.0221, 0.4321],\n",
      "        [0.5604, 0.3931, 0.8983],\n",
      "        [0.7991, 0.1926, 0.9275]])\n",
      "Normal random tensor: tensor([[ 0.6823, -1.2644, -0.0284],\n",
      "        [-0.4727,  0.1942,  0.2009],\n",
      "        [-1.0539, -1.9610, -1.2347]])\n",
      "Inger ranodm tenosr: tensor([[2, 2, 2],\n",
      "        [2, 2, 1],\n",
      "        [1, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generates a tensor with random numbers from a uniform distribution\n",
    "uniform_random_tensor = torch.rand(3, 3)  # Numbers between 0 and 1\n",
    "\n",
    "# Generates a tensor with random numbers from a normal distribution\n",
    "normal_random_tensor = torch.randn(3, 3)  # Numbers with mean 0 and variance 1\n",
    "\n",
    "# Generate a tensor of integer numbers for 1 to 3(not included), size = 3*3\n",
    "integer_random_tensor = torch.randint(1, 3, (3, 3))\n",
    "\n",
    "print(\"Uniform random tensor:\", uniform_random_tensor)\n",
    "print(\"Normal random tensor:\", normal_random_tensor)\n",
    "print(\"Inger ranodm tenosr:\", integer_random_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5754fa",
   "metadata": {},
   "source": [
    "### mean(axis = 0) vs mean(axis = 1) \n",
    "[same concept applicable  for other similar methods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b618bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean along axis 0 (average of columns): tensor([0.6831, 0.2026, 0.7527])\n",
      "Mean along axis 1 (average of rows): tensor([0.3813, 0.6173, 0.6397])\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean along axis 0 (collapsing rows, averaging columns)\n",
    "mean_axis_0 = uniform_random_tensor.mean(axis=0)\n",
    "\n",
    "# Compute the mean along axis 1 (collapsing columns, averaging rows)\n",
    "mean_axis_1 = uniform_random_tensor.mean(axis=1)\n",
    "\n",
    "print(\"Mean along axis 0 (average of columns):\", mean_axis_0)\n",
    "print(\"Mean along axis 1 (average of rows):\", mean_axis_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7d16b7",
   "metadata": {},
   "source": [
    "## Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f291c2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.0471, 10.1518, 20.9613])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "x = torch.tensor([5, 10, 20])\n",
    "y = torch.rand(3)\n",
    "print(x + y) #Position wise add\n",
    "\n",
    "# Matrix multiplication\n",
    "a = torch.ones(2, 3)\n",
    "b = torch.ones(3, 2)\n",
    "print(torch.matmul(a, b))\n",
    "\n",
    "# Element-wise multiplication\n",
    "print(a * b.t())  # .t() is for transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada84c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "print(w + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d29a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not possible due to dimention mitch match (must show en error)\n",
    "\n",
    "#print(w * b) \n",
    "#RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae7c5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4],\n",
       "         [5, 6]]),\n",
       " tensor([[1, 3, 5],\n",
       "         [2, 4, 6]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, b.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c0bc621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  6, 15],\n",
       "        [ 8, 20, 36]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now it has matching dimentions for element wise multiplication\n",
    "w * b.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d439a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not possible due to dimention mitch match (must show en error)\n",
    "# torch.matmul(w, b.t()) #RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x3 and 2x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a775ac",
   "metadata": {},
   "source": [
    "# Autograd: Automatic Differentiation\n",
    "PyTorch’s autograd feature allows you to calculate the gradient of a computation graph dynamically. This is useful for training neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c86cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6667, 0.6667, 0.6667])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with `requires_grad=True` to track computation\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "y = x * 2\n",
    "z = y.mean()\n",
    "\n",
    "# Calculate gradients\n",
    "z.backward()\n",
    "\n",
    "# Print gradients d(z)/dx\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eabd03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1646756029501/work/build/aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba0a81",
   "metadata": {},
   "source": [
    "x is a tensor with requires_grad=True, so PyTorch will track operations on it to compute gradients later.\n",
    "\n",
    "y is derived from x, and since x has requires_grad=True, operations on y are also tracked, and y inherits the requires_grad property from x.\n",
    "\n",
    "z is the mean of y, and it is a scalar (a single value), which makes it suitable for calling z.backward(), triggering the computation of gradients.\n",
    "\n",
    "When we call z.backward(), PyTorch computes the gradient of z with respect to x, which is stored in x.grad. However, y.grad and z.grad are not computed because:\n",
    "\n",
    "* By default, PyTorch only accumulates gradients for leaf nodes in the computation graph, where a leaf node is a node that has requires_grad=True and was created by the user, not as the result of an operation. In this case, x is a leaf node, but y and z are not, because they are the results of operations.\n",
    "\n",
    "* Gradients are not computed for non-scalar outputs by default. Since y and z are not leaf nodes and the backward operation is called on z, the gradients are only calculated for x. PyTorch does this to save memory and computation since gradients are typically only needed for the parameters we wish to optimize (like x in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a8158",
   "metadata": {},
   "source": [
    "## GPU Acceleration\n",
    "PyTorch can utilize GPUs to speed up its tensor computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2625b1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to('cuda')\n",
    "\n",
    "# Perform operations on GPU\n",
    "y = x * 2\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fe0d0",
   "metadata": {},
   "source": [
    "# Core PyTorch Modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd303c06",
   "metadata": {},
   "source": [
    "##  1. torch.nn: Building Blocks for Neural Networks\n",
    "The torch.nn module in PyTorch provides the necessary building blocks for designing and creating neural networks. These building blocks include layers, activation functions, and utilities for assembling and managing these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f4962b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f368a16",
   "metadata": {},
   "source": [
    "### 1.1 Layers\n",
    "\n",
    "Layers are the fundamental units that process data in a neural network. Common types of layers include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b731ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Layers (nn.Linear): Implements a fully connected linear layer.\n",
    "linear_layer = nn.Linear(10, 5) # nn.Linear(in_features=10, out_features=5) #10 inputs, 5 outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22fa075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional Layers (nn.Conv2d): Applies a 2D convolution over an input signal.\n",
    "conv_layer = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f3bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recurrent Layers (nn.LSTM, nn.GRU): Layers for processing sequential data.\n",
    "lstm_layers = nn.LSTM(input_size = 10, hidden_size = 20, num_layers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13b17c",
   "metadata": {},
   "source": [
    "### 1.2 Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns, some common example includes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0eec82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU (nn.ReLU): A common activation function that outputs the input if it's positive, otherwise, it outputs zero.\n",
    "relu = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7ff89c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid (nn.Sigmoid): Transforms inputs into a range between 0 and 1, often used for binary classification.\n",
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c15553b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax (nn.Softmax): Used for multi-class classification, turning logits into probabilities.\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6eff7",
   "metadata": {},
   "source": [
    "## 2. torch.optim: Optimization Algorithms for Training\n",
    "The torch.optim module provides implementations of various optimization algorithms used for training neural networks. These algorithms adjust the weights of the network to minimize the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b25101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416a092",
   "metadata": {},
   "source": [
    "### 2.1 Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e2a60b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD (Stochastic Gradient Descent): A basic yet effective optimization method.\n",
    "#optimization would be applied to model's parameter using assigned learning rate\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.01) #need to create the model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7569bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam: An optimizer that adapts the learning rate for each weight.\n",
    "#optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481464f3",
   "metadata": {},
   "source": [
    "#### 2.2 Using an Optimizer\n",
    "\n",
    "To use an optimizer, we need to perform the following steps in your training loop:\n",
    "\n",
    "* Zero the gradients (optimizer.zero_grad()).\n",
    "* Compute the loss.\n",
    "* Backpropagate (loss.backward()).\n",
    "* Update the model parameters (optimizer.step())."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18cf5f",
   "metadata": {},
   "source": [
    "## 3. torch.utils.data: Utilities for Data Loading and Preprocessing\n",
    "The torch.utils.data module provides tools for data loading and preprocessing, making it easier to feed data into your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdd79b",
   "metadata": {},
   "source": [
    "### 3.1 Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f18b34",
   "metadata": {},
   "source": [
    "### Dataset: \n",
    "An abstract class for representing a dataset. We can use built-in datasets (like CIFAR10, MNIST) or create our own custom dataset by subclassing Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6bfaaa",
   "metadata": {},
   "source": [
    "#### Built-in datset example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53d13c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# transformed_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# dataloader = DataLoader(transformed_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb6c4b",
   "metadata": {},
   "source": [
    "#### Custom dataset example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "031de154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41727793",
   "metadata": {},
   "source": [
    "The double underscores (often referred to as \"dunder\" methods) in Python indicate special methods that provide an interface for built-in behaviors or operations. \n",
    "To call these special methods, you usually don't invoke them directly using their names (like __len__() or __getitem__()). Instead, you use Python's built-in functions or operators that are designed to automatically trigger these methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eddf2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the custom dataset\n",
    "data = torch.randn(100, 10)  # Example data: 100 samples, 10 features each\n",
    "labels = torch.randint(0, 2, (100,))  # Example labels: 100 binary labels\n",
    "dataset = CustomDataset(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25dbaca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 100\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the dataset\n",
    "#We call __len__ indirectly by using the len() function on an instance of the dataset class.\n",
    "\n",
    "print(f\"Dataset length: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d005fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data point: tensor([ 0.7799, -0.5421,  1.6364, -0.6426,  0.5164,  0.0960, -0.2462, -2.3722,\n",
      "        -2.0264,  3.5598]), Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Access a single data point \n",
    "# we call __getitem__ by using the indexing syntax on the dataset instance.\n",
    "\n",
    "item, label = dataset[5]\n",
    "print(f\"Sample data point: {item}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17039a9",
   "metadata": {},
   "source": [
    "### DataLoader: \n",
    "Combines a dataset and a sampler, providing an iterable over the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da9fc810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Data torch.Size([15, 10]), Labels torch.Size([15])\n",
      "Batch 1: Data torch.Size([15, 10]), Labels torch.Size([15])\n",
      "Batch 2: Data torch.Size([15, 10]), Labels torch.Size([15])\n",
      "Batch 3: Data torch.Size([15, 10]), Labels torch.Size([15])\n",
      "Batch 4: Data torch.Size([15, 10]), Labels torch.Size([15])\n",
      "Batch 5: Data torch.Size([15, 10]), Labels torch.Size([15])\n",
      "Batch 6: Data torch.Size([10, 10]), Labels torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Using the dataset with DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size = 15, shuffle = True)\n",
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}: Data {data.size()}, Labels {labels.size()}\")\n",
    "    # Here we would typically proceed with processing the data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba72460",
   "metadata": {},
   "source": [
    "## 3.2 Preprocessing\n",
    "\n",
    "Preprocessing steps like normalization and data augmentation can be integrated with dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9460a6",
   "metadata": {},
   "source": [
    "#### Example with image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30c7b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load a sample dataset (e.g., CIFAR10)\n",
    "# dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9c0d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Resize(256),  # Resize images to 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Crop images to 224x224 pixels\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize images\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "211d1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations to the dataset\n",
    "# transformed_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f553d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cfebdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the data loader\n",
    "for images, labels in dataloader:\n",
    "    # Here, we would typically feed 'images' and 'labels' to the model\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a0735",
   "metadata": {},
   "source": [
    "#### Example: Preprocessing Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69ced143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.374540</td>\n",
       "      <td>0.950714</td>\n",
       "      <td>0.731994</td>\n",
       "      <td>0.598658</td>\n",
       "      <td>0.156019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.155995</td>\n",
       "      <td>0.058084</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>0.601115</td>\n",
       "      <td>0.708073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020584</td>\n",
       "      <td>0.969910</td>\n",
       "      <td>0.832443</td>\n",
       "      <td>0.212339</td>\n",
       "      <td>0.181825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  target\n",
       "0  0.374540  0.950714  0.731994  0.598658  0.156019       1\n",
       "1  0.155995  0.058084  0.866176  0.601115  0.708073       0\n",
       "2  0.020584  0.969910  0.832443  0.212339  0.181825       0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "data = np.random.rand(100,5) # 100 samples and 5 features\n",
    "labels = np.random.randint(0, 2, (100,)) #binary targets\n",
    "columns = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n",
    "\n",
    "#Convert to a Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns = columns)\n",
    "df['target'] = labels\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7ed2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Data into Features and Labels\n",
    "X = df.drop('target', axis = 1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "453f5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features to have a mean of 0 and a standard deviation of 1.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac5d81a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Define a custom Dataset class that will be used with PyTorch's DataLoader.\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype = torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype = torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23465477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the custom dataset\n",
    "dataset = TabularDataset(X_normalized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "053a2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create DataLoader \n",
    "dataloader = DataLoader(dataset, batch_size = 10, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dc5bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n",
      "Features shape: torch.Size([10, 5]), Labels shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#using dataloader: iterate over the DataLoader in a training loop\n",
    "for epoch in range(2):  # example for 2 epochs\n",
    "    for features, labels in dataloader:\n",
    "        # Here, we would typically feed 'features' and 'labels' to the model\n",
    "        print(f\"Features shape: {features.shape}, Labels shape: {labels.shape}\")\n",
    "        # Simulate a training step\n",
    "        # e.g., loss = model(features, labels)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d8c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
